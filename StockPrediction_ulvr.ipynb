{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83ff98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb36c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>4201.5</td>\n",
       "      <td>4232.5</td>\n",
       "      <td>4175.5</td>\n",
       "      <td>4232.5</td>\n",
       "      <td>4081.537598</td>\n",
       "      <td>2717881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>4248.5</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>4222.5</td>\n",
       "      <td>4223.5</td>\n",
       "      <td>4072.858643</td>\n",
       "      <td>5825908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-05</td>\n",
       "      <td>4233.0</td>\n",
       "      <td>4288.5</td>\n",
       "      <td>4222.5</td>\n",
       "      <td>4288.5</td>\n",
       "      <td>4135.539551</td>\n",
       "      <td>3772390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>4278.0</td>\n",
       "      <td>4329.0</td>\n",
       "      <td>4266.5</td>\n",
       "      <td>4307.5</td>\n",
       "      <td>4153.862793</td>\n",
       "      <td>3117901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>4306.0</td>\n",
       "      <td>4314.5</td>\n",
       "      <td>4274.0</td>\n",
       "      <td>4297.0</td>\n",
       "      <td>4143.737305</td>\n",
       "      <td>2920574.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open    High     Low   Close    Adj Close     Volume\n",
       "0  2023-04-03  4201.5  4232.5  4175.5  4232.5  4081.537598  2717881.0\n",
       "1  2023-04-04  4248.5  4259.0  4222.5  4223.5  4072.858643  5825908.0\n",
       "2  2023-04-05  4233.0  4288.5  4222.5  4288.5  4135.539551  3772390.0\n",
       "3  2023-04-06  4278.0  4329.0  4266.5  4307.5  4153.862793  3117901.0\n",
       "4  2023-04-11  4306.0  4314.5  4274.0  4297.0  4143.737305  2920574.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ULVR.L.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b402bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.iloc[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "845dd607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>4201.5</td>\n",
       "      <td>4232.5</td>\n",
       "      <td>4175.5</td>\n",
       "      <td>4232.5</td>\n",
       "      <td>4081.537598</td>\n",
       "      <td>2717881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-04</td>\n",
       "      <td>4248.5</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>4222.5</td>\n",
       "      <td>4223.5</td>\n",
       "      <td>4072.858643</td>\n",
       "      <td>5825908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-05</td>\n",
       "      <td>4233.0</td>\n",
       "      <td>4288.5</td>\n",
       "      <td>4222.5</td>\n",
       "      <td>4288.5</td>\n",
       "      <td>4135.539551</td>\n",
       "      <td>3772390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>4278.0</td>\n",
       "      <td>4329.0</td>\n",
       "      <td>4266.5</td>\n",
       "      <td>4307.5</td>\n",
       "      <td>4153.862793</td>\n",
       "      <td>3117901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-11</td>\n",
       "      <td>4306.0</td>\n",
       "      <td>4314.5</td>\n",
       "      <td>4274.0</td>\n",
       "      <td>4297.0</td>\n",
       "      <td>4143.737305</td>\n",
       "      <td>2920574.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open    High     Low   Close    Adj Close     Volume\n",
       "0  2023-04-03  4201.5  4232.5  4175.5  4232.5  4081.537598  2717881.0\n",
       "1  2023-04-04  4248.5  4259.0  4222.5  4223.5  4072.858643  5825908.0\n",
       "2  2023-04-05  4233.0  4288.5  4222.5  4288.5  4135.539551  3772390.0\n",
       "3  2023-04-06  4278.0  4329.0  4266.5  4307.5  4153.862793  3117901.0\n",
       "4  2023-04-11  4306.0  4314.5  4274.0  4297.0  4143.737305  2920574.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93bed97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.drop(['Date '], axis=1)\n",
    "# df=df.drop(['series '], axis=1)\n",
    "# df=df.drop(['PREV. CLOSE '], axis=1)\n",
    "# df=df.drop(['ltp '], axis=1)\n",
    "# df=df.drop(['vwap '], axis=1)\n",
    "# df=df.drop(['VALUE '], axis=1)\n",
    "# df=df.drop(['No of trades '], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f235717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>4201.5</td>\n",
       "      <td>4232.5</td>\n",
       "      <td>4175.5</td>\n",
       "      <td>4232.5</td>\n",
       "      <td>4081.537598</td>\n",
       "      <td>2717881.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-04</th>\n",
       "      <td>4248.5</td>\n",
       "      <td>4259.0</td>\n",
       "      <td>4222.5</td>\n",
       "      <td>4223.5</td>\n",
       "      <td>4072.858643</td>\n",
       "      <td>5825908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-05</th>\n",
       "      <td>4233.0</td>\n",
       "      <td>4288.5</td>\n",
       "      <td>4222.5</td>\n",
       "      <td>4288.5</td>\n",
       "      <td>4135.539551</td>\n",
       "      <td>3772390.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-06</th>\n",
       "      <td>4278.0</td>\n",
       "      <td>4329.0</td>\n",
       "      <td>4266.5</td>\n",
       "      <td>4307.5</td>\n",
       "      <td>4153.862793</td>\n",
       "      <td>3117901.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-11</th>\n",
       "      <td>4306.0</td>\n",
       "      <td>4314.5</td>\n",
       "      <td>4274.0</td>\n",
       "      <td>4297.0</td>\n",
       "      <td>4143.737305</td>\n",
       "      <td>2920574.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High     Low   Close    Adj Close     Volume\n",
       "Date                                                              \n",
       "2023-04-03  4201.5  4232.5  4175.5  4232.5  4081.537598  2717881.0\n",
       "2023-04-04  4248.5  4259.0  4222.5  4223.5  4072.858643  5825908.0\n",
       "2023-04-05  4233.0  4288.5  4222.5  4288.5  4135.539551  3772390.0\n",
       "2023-04-06  4278.0  4329.0  4266.5  4307.5  4153.862793  3117901.0\n",
       "2023-04-11  4306.0  4314.5  4274.0  4297.0  4143.737305  2920574.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57eb8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 251 entries, 2023-04-03 to 2024-04-02\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Open       250 non-null    float64\n",
      " 1   High       250 non-null    float64\n",
      " 2   Low        250 non-null    float64\n",
      " 3   Close      250 non-null    float64\n",
      " 4   Adj Close  250 non-null    float64\n",
      " 5   Volume     250 non-null    float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 13.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "629206e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(dat):\n",
    "    d=dat.replace(\",\",\"\")\n",
    "    d=float(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7acf6601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int(dat):\n",
    "    d=dat.replace(\",\",\"\")\n",
    "    d=int(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b1af026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['OPEN '] = df['OPEN '].apply(convert)\n",
    "# df['HIGH '] = df['HIGH '].apply(convert)\n",
    "# df['LOW '] = df['LOW '].apply(convert)\n",
    "# df['close '] = df['close '].apply(convert)\n",
    "# df['52W H '] = df['52W H '].apply(convert)\n",
    "# df['52W L '] = df['52W L '].apply(convert)\n",
    "#df['VOLUME '] = df['VOLUME '].apply(convert_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64c2d85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 251 entries, 2023-04-03 to 2024-04-02\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Open       250 non-null    float64\n",
      " 1   High       250 non-null    float64\n",
      " 2   Low        250 non-null    float64\n",
      " 3   Close      250 non-null    float64\n",
      " 4   Adj Close  250 non-null    float64\n",
      " 5   Volume     250 non-null    float64\n",
      "dtypes: float64(6)\n",
      "memory usage: 13.7 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e798f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>2.500000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4006.988000</td>\n",
       "      <td>4031.450709</td>\n",
       "      <td>3981.421640</td>\n",
       "      <td>4004.678000</td>\n",
       "      <td>3931.945477</td>\n",
       "      <td>4.688923e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>173.788635</td>\n",
       "      <td>172.944070</td>\n",
       "      <td>173.640076</td>\n",
       "      <td>172.981925</td>\n",
       "      <td>137.416176</td>\n",
       "      <td>2.386460e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3694.500000</td>\n",
       "      <td>3723.000000</td>\n",
       "      <td>3680.500000</td>\n",
       "      <td>3694.000000</td>\n",
       "      <td>3660.772705</td>\n",
       "      <td>9.394910e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3874.000000</td>\n",
       "      <td>3895.625000</td>\n",
       "      <td>3847.250000</td>\n",
       "      <td>3862.625000</td>\n",
       "      <td>3828.401001</td>\n",
       "      <td>3.083393e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3997.500000</td>\n",
       "      <td>4023.500000</td>\n",
       "      <td>3976.250000</td>\n",
       "      <td>3995.250000</td>\n",
       "      <td>3924.128907</td>\n",
       "      <td>4.214757e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4088.500000</td>\n",
       "      <td>4106.855591</td>\n",
       "      <td>4062.250000</td>\n",
       "      <td>4082.375000</td>\n",
       "      <td>4001.500000</td>\n",
       "      <td>5.493132e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4475.000000</td>\n",
       "      <td>4483.250000</td>\n",
       "      <td>4433.000000</td>\n",
       "      <td>4443.500000</td>\n",
       "      <td>4285.011719</td>\n",
       "      <td>1.459145e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close    Adj Close  \\\n",
       "count   250.000000   250.000000   250.000000   250.000000   250.000000   \n",
       "mean   4006.988000  4031.450709  3981.421640  4004.678000  3931.945477   \n",
       "std     173.788635   172.944070   173.640076   172.981925   137.416176   \n",
       "min    3694.500000  3723.000000  3680.500000  3694.000000  3660.772705   \n",
       "25%    3874.000000  3895.625000  3847.250000  3862.625000  3828.401001   \n",
       "50%    3997.500000  4023.500000  3976.250000  3995.250000  3924.128907   \n",
       "75%    4088.500000  4106.855591  4062.250000  4082.375000  4001.500000   \n",
       "max    4475.000000  4483.250000  4433.000000  4443.500000  4285.011719   \n",
       "\n",
       "             Volume  \n",
       "count  2.500000e+02  \n",
       "mean   4.688923e+06  \n",
       "std    2.386460e+06  \n",
       "min    9.394910e+05  \n",
       "25%    3.083393e+06  \n",
       "50%    4.214757e+06  \n",
       "75%    5.493132e+06  \n",
       "max    1.459145e+07  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a98fa52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open         1\n",
       "High         1\n",
       "Low          1\n",
       "Close        1\n",
       "Adj Close    1\n",
       "Volume       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "007795d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Next_Day_Close'] = df['Close'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60ed1ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Next_Day_Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-03-25</th>\n",
       "      <td>3963.5</td>\n",
       "      <td>3980.0</td>\n",
       "      <td>3954.999023</td>\n",
       "      <td>3957.5</td>\n",
       "      <td>3957.5</td>\n",
       "      <td>3301289.0</td>\n",
       "      <td>3966.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-26</th>\n",
       "      <td>3934.5</td>\n",
       "      <td>3976.5</td>\n",
       "      <td>3926.500000</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>6152546.0</td>\n",
       "      <td>3962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-27</th>\n",
       "      <td>3949.0</td>\n",
       "      <td>3973.0</td>\n",
       "      <td>3931.500000</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>5847297.0</td>\n",
       "      <td>3975.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>3974.5</td>\n",
       "      <td>3982.5</td>\n",
       "      <td>3956.000000</td>\n",
       "      <td>3975.5</td>\n",
       "      <td>3975.5</td>\n",
       "      <td>3456269.0</td>\n",
       "      <td>3935.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-04-02</th>\n",
       "      <td>3976.0</td>\n",
       "      <td>3979.0</td>\n",
       "      <td>3930.499023</td>\n",
       "      <td>3935.0</td>\n",
       "      <td>3935.0</td>\n",
       "      <td>4329133.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High          Low   Close  Adj Close     Volume  \\\n",
       "Date                                                                    \n",
       "2024-03-25  3963.5  3980.0  3954.999023  3957.5     3957.5  3301289.0   \n",
       "2024-03-26  3934.5  3976.5  3926.500000  3966.0     3966.0  6152546.0   \n",
       "2024-03-27  3949.0  3973.0  3931.500000  3962.0     3962.0  5847297.0   \n",
       "2024-03-28  3974.5  3982.5  3956.000000  3975.5     3975.5  3456269.0   \n",
       "2024-04-02  3976.0  3979.0  3930.499023  3935.0     3935.0  4329133.0   \n",
       "\n",
       "            Next_Day_Close  \n",
       "Date                        \n",
       "2024-03-25          3966.0  \n",
       "2024-03-26          3962.0  \n",
       "2024-03-27          3975.5  \n",
       "2024-03-28          3935.0  \n",
       "2024-04-02             NaN  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12e6e618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(251, 7)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b923334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop([df.shape[0]-1], axis=0, inplace=True)\n",
    "df = df.drop(index=\"2024-04-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5ed3f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Next_Day_Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-03-22</th>\n",
       "      <td>3931.0</td>\n",
       "      <td>3993.0</td>\n",
       "      <td>3923.000000</td>\n",
       "      <td>3976.5</td>\n",
       "      <td>3976.5</td>\n",
       "      <td>11423811.0</td>\n",
       "      <td>3957.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-25</th>\n",
       "      <td>3963.5</td>\n",
       "      <td>3980.0</td>\n",
       "      <td>3954.999023</td>\n",
       "      <td>3957.5</td>\n",
       "      <td>3957.5</td>\n",
       "      <td>3301289.0</td>\n",
       "      <td>3966.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-26</th>\n",
       "      <td>3934.5</td>\n",
       "      <td>3976.5</td>\n",
       "      <td>3926.500000</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>3966.0</td>\n",
       "      <td>6152546.0</td>\n",
       "      <td>3962.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-27</th>\n",
       "      <td>3949.0</td>\n",
       "      <td>3973.0</td>\n",
       "      <td>3931.500000</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>3962.0</td>\n",
       "      <td>5847297.0</td>\n",
       "      <td>3975.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-03-28</th>\n",
       "      <td>3974.5</td>\n",
       "      <td>3982.5</td>\n",
       "      <td>3956.000000</td>\n",
       "      <td>3975.5</td>\n",
       "      <td>3975.5</td>\n",
       "      <td>3456269.0</td>\n",
       "      <td>3935.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High          Low   Close  Adj Close      Volume  \\\n",
       "Date                                                                     \n",
       "2024-03-22  3931.0  3993.0  3923.000000  3976.5     3976.5  11423811.0   \n",
       "2024-03-25  3963.5  3980.0  3954.999023  3957.5     3957.5   3301289.0   \n",
       "2024-03-26  3934.5  3976.5  3926.500000  3966.0     3966.0   6152546.0   \n",
       "2024-03-27  3949.0  3973.0  3931.500000  3962.0     3962.0   5847297.0   \n",
       "2024-03-28  3974.5  3982.5  3956.000000  3975.5     3975.5   3456269.0   \n",
       "\n",
       "            Next_Day_Close  \n",
       "Date                        \n",
       "2024-03-22          3957.5  \n",
       "2024-03-25          3966.0  \n",
       "2024-03-26          3962.0  \n",
       "2024-03-27          3975.5  \n",
       "2024-03-28          3935.0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6baeec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=df[0:int(len(df) * 0.8)]\n",
    "test_data=df[int(len(df) * 0.8):]\n",
    "train_ar = train_data['Next_Day_Close'].values\n",
    "test_ar = test_data['Next_Day_Close'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e9cb2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_data.drop(['Next_Day_Close'], axis=1)\n",
    "X_test=test_data.drop(['Next_Day_Close'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64809120",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.expand_dims(X_train,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0125fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.expand_dims(X_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a1089c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_data['Next_Day_Close']\n",
    "y_test=test_data['Next_Day_Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2e0bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d8cf932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_2 (LSTM)               (None, 1, 50)             11400     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1, 50)             0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,651\n",
      "Trainable params: 31,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape = (1,X_train.shape[2],)))\n",
    "\n",
    "model.add(Dropout(0.1)) \n",
    "model.add(LSTM(units=50))\n",
    "\n",
    "model.add(Dense(1,activation=\"linear\"))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b72b9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'model_ULVR.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,monitor='loss',verbose=1,save_best_only=True,mode='min')\n",
    "ES = EarlyStopping(monitor='loss', patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "084ed196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/7 [===>..........................] - ETA: 21s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 1: loss did not improve from inf\n",
      "7/7 [==============================] - 5s 177ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 2/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 2: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 3/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 3: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 4/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 4: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 5/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 5: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 6/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 6: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 12ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 7/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 7: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 8/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 8: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 9/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 9: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 10/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 10: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 11/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 11: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 12/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 12: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 13/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 13: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 10ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 14/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 14: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 15/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 15: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 16/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 16: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 17/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 17: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 18/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 18: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 19/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 19: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 20/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 20: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 21/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 21: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 22/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 22: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 23/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 23: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 24/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 24: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 25/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 25: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 26/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 26: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 27/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 27: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 28/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 28: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 29/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 29: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 30/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 30: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 31: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 32/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 32: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 33/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 33: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 34/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 34: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 35/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 35: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 36/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 36: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 37/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 37: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 38/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 38: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 39/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 39: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 40/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 40: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 41/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 41: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 42/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 42: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 43/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 43: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 44/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 44: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 45/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 45: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 46/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 46: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 11ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 47/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 47: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 48/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 48: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 49/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 49: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 50/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 50: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 51/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 51: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 52/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 52: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 53/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 53: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 54/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 54: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 55/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 55: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 56/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 56: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 57/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 57: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 58/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 58: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 59/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 59: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 60/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 60: loss did not improve from inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 9ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 61/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 61: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 62/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 62: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 63/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 63: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 64/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 64: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 65/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 65: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 66/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 66: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 67/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 67: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 68/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 68: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 69/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 69: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 70/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 70: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 71/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 71: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 72/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 72: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 73/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 73: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 74/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 74: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 75/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 75: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 76/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 76: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 77/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 77: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 78/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 78: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 79/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 79: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 80/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 80: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 81/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 81: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 82/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 82: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 83/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 83: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 6ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 84/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 84: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 85/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 85: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 86/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 86: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 9ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 87/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 87: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 88/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 88: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 89/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 89: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 90: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 91/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 91: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 92/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 92: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 93/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 93: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 94/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 94: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 95/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 95: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 96/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 96: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 97/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 97: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 98/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 98: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 8ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 99/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 99: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n",
      "Epoch 100/10000\n",
      "1/7 [===>..........................] - ETA: 0s - loss: nan - mean_absolute_error: nan\n",
      "Epoch 100: loss did not improve from inf\n",
      "7/7 [==============================] - 0s 7ms/step - loss: nan - mean_absolute_error: nan - val_loss: nan - val_mean_absolute_error: nan\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train, y_train, epochs=10000,validation_data=[X_test,y_test], callbacks = [checkpoint,ES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "30e57a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6b157a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.title('Training accuracy and loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Loss', 'mean_absolute_error'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38ce865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e7bd77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
